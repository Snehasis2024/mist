launcher_args: {experiment_name: 2022_10_31_canopus_forward,
  script_name: "run_scripts/train_forward_ffn.py",
  slurm_script: launcher_scripts/generic_slurm.sh, 
  launch_method: local_parallel,
  visible_devices: [0, 1,2,3]
}
universal_args:
  _slurm_args:
  - {_num_gpu: 1, cpus-per-task: 7, job-name: forward_train, mem-per-cpu: 8G, #nodelist: 'node[1236]',
    time: '1-18:00:00'} # Use less time for slurm constraint
  debug: [false] #[true] #[false]
  gpu: [true] #[true]
  scheduler: [false]
  learning-rate: [7.0e-4]
  lr-decay-rate: [0.99]
  seed: [42]
  num-workers: [16]
  batch-size: [64]
  max-epochs: [300]
  dataset-name: [canopus_train]
  split-name: [ # Model ablations
              canopus_hplus_100_0.csv, 
              canopus_hplus_100_1.csv, 
              canopus_hplus_100_2.csv,

               # Data ablations
               canopus_hplus_20_0.csv, 
               canopus_hplus_40_0.csv, 
               canopus_hplus_60_0.csv, 
               canopus_hplus_80_0.csv]
  num-bins: [15000] 
  upper-limit: [1500]
  layers: [3]
  dropout: [0.3]
  hidden-size: [200]
  use-reverse: [true]

iterative_args:
  - use-reverse: [true]
    scheduler: [true]
    learning-rate: [0.00055]
    dropout: [0.1]
    layers: [2]
    hidden-size: [512]
    overfit-train: [false]
    loss-fn: ["bce"]
    fp-type: [morgan4096_2]
    growing: [iterative]
    growing-weight: [0.0016]
    growing-scheme: [interleave]
    growing-layers: [3]
